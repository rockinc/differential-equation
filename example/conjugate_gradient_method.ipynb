{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共役勾配法で連立方程式の解を計算しよう\n",
    "\n",
    "共役勾配法とは，連立方程式の解を，コスト関数の最小化問題に落とし込むことで計算する手法である．\n",
    "以下の方程式を解くことを考える．\n",
    "\\begin{eqnarray}\n",
    "    Ax = b\n",
    "\\end{eqnarray}\n",
    "ただし，$A$は対称な正定値行列である．\n",
    "\n",
    "このとき，連立方程式の解は，\n",
    "\\begin{eqnarray}\n",
    "    f(x) = \\frac{1}{2}x^TAx - b^T x\n",
    "\\end{eqnarray}\n",
    "を最小化する$x$と等しくなる．これは，$A$が対称行列のとき，\n",
    "\\begin{eqnarray}\n",
    "    \\frac{d f(x)}{dx_i} = a_{ii}x_i + \\frac{1}{2}\\sum_{k\\neq i}^{}(a_{ik} + a_{ki}{})x_k - b = (Ax-b)_ i = 0\\\\\n",
    "    \\therefore \\frac{d f(x)}{dx} = Ax-b = 0\n",
    "\\end{eqnarray}\n",
    "が$f(x)$の最適化条件だからである．\n",
    "\n",
    "> $A$が正定値でないと，$Ax-b=0$が$f(x)$の最適化条件でなくなる．\n",
    "\n",
    "## アルゴリズムの手順\n",
    "$x_k$を$k$回目の(後に説明する)イテレーションで求めた解$x$とする．\n",
    "\n",
    "\n",
    "$x_0=0$を準備する．$k=0$から次の操作を$x_{k}$が収束するまでイテレーションを繰り返すことで，解を計算する．\n",
    "\\begin{eqnarray}\n",
    "    r_k = b-Ax_k\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "    p_{k} = r_k  - \\sum_{i=0}^{k-1} \\frac{p_i^T A r_k}{p_i A p_i} p_i \n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "    \\alpha_k = \\frac{p_k^T r_k}{p_k^T A p_k}\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "    x_{k+1} = \\alpha_k p_k + x_k\n",
    "\\end{eqnarray}\n",
    "\n",
    "### $\\alpha_k$や$p_k,x_k$の漸化式の意味\n",
    "\n",
    "#### $\\alpha_k$\n",
    "$\\alpha_k$は，$k+1$回目のイテレーションで$f(x)$を最小化する係数である．\n",
    "\\begin{eqnarray}\n",
    "    f(x_{k+1}) = f(x+\\alpha_k p_k) = \\frac{1}{2}(x+\\alpha_k p_k)^T A (x+\\alpha_k p_k) - b^T (x+\\alpha_k p_k)\\\\\n",
    "    \\frac{\\partial f(x_{k+1})}{\\partial \\alpha_k} = \\alpha_k p_k^T A p_k + p_k^T A x_k - p_k^T b\n",
    "\\end{eqnarray}\n",
    "なので，$f(x_{k+1})$を最適化するような$\\alpha_k$は，\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial f(x_{k+1})}{\\partial \\alpha_k} = 0\\\\\n",
    "    \\therefore \\alpha_k = \\frac{p_k^T (b-A x_k)}{p_k^T A p_k} = \\frac{p_k^T r_k}{p_k^T A p_k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### $p_k$\n",
    "共役法では，求めたい解$x$を$A$に対して互いに共役なベクトルの線形結合で表しその結合係数を計算することで，解を計算する．共役勾配法も共役法と同様な計算を行っている．\n",
    "$p_k$と$p_j(j<k)$が$A$に対して互いに共役であることは次の計算により確かめることができる．\n",
    "\\begin{eqnarray}\n",
    "    p_k^T A p_j = \\left(r_k  - \\sum_{i=0}^{k-1} \\frac{p_i^T A r_k}{p_i A p_i} p_i \\right)^T A p_i = r_k^T Ap_j - \\frac{p_j^T A r_k}{p_j^T A p_j}p_j^TAp_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### $x_k$の漸化式\n",
    "$\\alpha_k$を$x_k$に足して$x$を更新することで，$f(x_k)$の最小の値を計算することができる．$\\alpha_k$をその都度のコスト関数の最適化によって求めることができる．\n",
    "> $f(x_k)$が単調減少する証明は存在する？→はい，$a_k=0$にすれば$f(x_{k+1})=f(x_k)$とすることができるので，$f(x_{k+1})\\leq f(x_k)$である．また$f(x)$は放物面なので$f(x)$が最適化されれば，必然的に$x$は厳密解に近づくと考えられる．\n",
    "\n",
    "### 注釈\n",
    "> 文字の定義の仕方\n",
    "> 英語大文字が行列，英語小文字がベクトル，ギリシャ文字がスカラーである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def conjugate_gradient(mat_A,vec_b):\n",
    "    \"\"\"\n",
    "    :param mat_A: `np.ndarray` matrix A\n",
    "    :param vec_b: `np.ndarray` vector b\n",
    "    :return: `np.ndarray` solution x\n",
    "    \"\"\"\n",
    "    size = len(vec_b)\n",
    "    vec_r = np.zeros(size)\n",
    "    vec_x = np.zeros(size)\n",
    "    list_vec_p = []\n",
    "    for k in range(size):\n",
    "        vec_r = vec_b-mat_A@vec_x\n",
    "        vec_d = np.zeros(len(mat_A))\n",
    "        for i in range(k):\n",
    "            vec_d += (list_vec_p[i] @ mat_A @ vec_r)/(list_vec_p[i] @ mat_A @ list_vec_p[i]) * list_vec_p[i]\n",
    "        vec_p = vec_r - vec_d\n",
    "        alpha = (vec_p @ vec_r)/(vec_p@mat_A@vec_p)\n",
    "        vec_x = alpha*vec_p + vec_x\n",
    "        list_vec_p.append(vec_p)\n",
    "        #print(\"r\",vec_r)\n",
    "        #print(\"x\",vec_x)\n",
    "        #print(\"d\",vec_d)\n",
    "        #print(alpha)\n",
    "    return vec_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact: [0.45454545 0.18181818]\n",
      "Conjugate Gradient: [0.45454545 0.18181818]\n",
      "Exact: [-2.  -1.5  4.5]\n",
      "Conjugate Gradient: [-2.  -1.5  4.5]\n",
      "Condition Number of A1: 1.938748901931751\n",
      "Eigs of A1: [4.61803399 2.38196601]\n",
      "Condition Number of A2: 21.424394502595273\n",
      "Eigs of A2: [10.06695818 -2.53684115  0.46988297]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.array([[4,1],[1,3]])\n",
    "A2 = np.array([[2,5,3],[5,3,3],[3,3,3]])\n",
    "b1 = np.array([2,1])\n",
    "b2 = np.array([2,-1,3])\n",
    "print(f\"Exact: {np.linalg.inv(A1) @ b1}\")\n",
    "print(f\"Conjugate Gradient: {conjugate_gradient(mat_A=A1,vec_b=b1)}\")\n",
    "\n",
    "print(f\"Exact: {np.linalg.inv(A2) @ b2}\")\n",
    "print(f\"Conjugate Gradient: {conjugate_gradient(mat_A=A2,vec_b=b2)}\")\n",
    "\n",
    "## 条件数\n",
    "print(f\"Condition Number of A1: {np.linalg.cond(A1)}\")\n",
    "print(f\"Eigs of A1: {np.linalg.eig(A1)[0]}\")\n",
    "\n",
    "print(f\"Condition Number of A2: {np.linalg.cond(A2)}\")\n",
    "print(f\"Eigs of A2: {np.linalg.eig(A2)[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## より賢い実装(解説と証明工事中)\n",
    "先ほどの実装では$\\vec{p}$を更新する際の$\\vec{d}$の計算で一回のイテレーションで，$O(N^3)$の計算コストが必要とし，イテレーションを$O(N)$繰り返すとするとアルゴリズムトータルでは$O(N^4)$の計算コストである．これはガウスの消去法よりも遅い．そこで$\\vec{p}$を更新する部分の高速化を検討する．"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def conjugate_gradient_rev(mat_A,vec_b):\n",
    "    size = len(vec_b)\n",
    "    vec_x = np.zeros(len(mat_A))\n",
    "    vec_r = vec_b - mat_A@vec_x\n",
    "    vec_p = vec_b - mat_A@vec_x\n",
    "    for k in range(100):\n",
    "        r_norm = vec_r@vec_r\n",
    "        if r_norm<=10**-10:\n",
    "            return vec_x\n",
    "        vec_Ap = mat_A@vec_p\n",
    "        alpha = r_norm/(vec_p@vec_Ap)\n",
    "        vec_x += alpha*vec_p\n",
    "        vec_rn = vec_r-alpha*vec_Ap\n",
    "        beta = (vec_rn@vec_rn)/r_norm\n",
    "        vec_r -= alpha*vec_Ap\n",
    "        vec_p = vec_r + beta*vec_p\n",
    "    return vec_x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact: [0.45454545 0.18181818]\n",
      "Conjugate Gradient: [0.45454545 0.18181818]\n",
      "Exact: [0.75   0.3125 0.0625]\n",
      "Conjugate Gradient: [0.75   0.3125 0.0625]\n",
      "Condition Number of A1: 1.938748901931751\n",
      "Eigs of A1: [4.61803399 2.38196601]\n",
      "Condition Number of A2: 7.089377080166892\n",
      "Eigs of A2: [ 5.39754782 -0.91447682 -6.483071  ]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.array([[4,1],[1,3]])\n",
    "A2 = np.array([[2,1,3],[1,-6,2],[3,2,2]])\n",
    "b1 = np.array([2,1])\n",
    "b2 = np.array([2,-1,3])\n",
    "print(f\"Exact: {np.linalg.inv(A1) @ b1}\")\n",
    "print(f\"Conjugate Gradient: {conjugate_gradient_rev(mat_A=A1,vec_b=b1)}\")\n",
    "\n",
    "print(f\"Exact: {np.linalg.inv(A2) @ b2}\")\n",
    "print(f\"Conjugate Gradient: {conjugate_gradient_rev(mat_A=A2,vec_b=b2)}\")\n",
    "\n",
    "## 条件数\n",
    "print(f\"Condition Number of A1: {np.linalg.cond(A1)}\")\n",
    "print(f\"Eigs of A1: {np.linalg.eig(A1)[0]}\")\n",
    "\n",
    "print(f\"Condition Number of A2: {np.linalg.cond(A2)}\")\n",
    "print(f\"Eigs of A2: {np.linalg.eig(A2)[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 時間の比較"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "size = 1000\n",
    "a = np.arange(1,size*size+1).reshape(size,size)\n",
    "L = np.tril(a)\n",
    "A = L@L.T\n",
    "b = np.arange(size)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "conjugate_gradient(mat_A=A,vec_b=b)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "conjugate_gradient_rev(mat_A=A,vec_b=b)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(t1-t0,t2-t1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}